# Transformer
Transformer model architecture crafted from scratch in Tensorflow. Use it for generating sequence using embeddings data.

Transformer block based on the research paper "Attention is all you need", created in Tensorflow.

Code includes Positional Encoding, Encoder Block, Decoder Block, Multi Head Attention, Transformer Block architecture.
This can be used in Generative AI related tasks.
